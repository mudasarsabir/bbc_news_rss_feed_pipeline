{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20e5fb63-78ac-4492-9829-8110196e1a57",
   "metadata": {},
   "source": [
    "# RSS Feed Data Pipeline: Extracting, Analyzing, and Storing BBC News 📡📰\n",
    "\n",
    "Welcome to this project where we explore the powerful world of data pipelines using **RSS Feeds**. In this project, we'll focus on **BBC News**, extracting the latest articles, analyzing the data, and storing it in a structured format for future use.\n",
    "\n",
    "---\n",
    "\n",
    "## Project Overview 🔍\n",
    "\n",
    "This project demonstrates how to:\n",
    "1. **Extract Data**: Parse BBC News RSS feeds to retrieve the latest articles.\n",
    "2. **Analyze Data**: Clean, process, and analyze the extracted data for meaningful insights.\n",
    "3. **Store Data**: Save the processed data in a structured format (CSV) for easy access and future analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Features 💡\n",
    "- **RSS Feed Parsing**: Using the `feedparser` library to efficiently extract article details like title, link, publication date, and category.\n",
    "- **Data Processing**: Cleaning and organizing the data for further analysis using Python’s **pandas** library.\n",
    "- **Storage**: Storing the cleaned data in CSV format, which can be easily shared or loaded into other applications.\n",
    "\n",
    "---\n",
    "\n",
    "## Workflow 📈\n",
    "\n",
    "1. **RSS Feed Parsing**: We start by fetching the RSS feed from BBC News and parsing it into a structured format.\n",
    "2. **Data Cleaning**: After extracting the data, we handle missing values, remove duplicates, and standardize the data.\n",
    "3. **Analysis**: Analyze the frequency of article categories, trends in news coverage, and other insights.\n",
    "4. **Data Storage**: Finally, the clean and structured data is stored in a CSV file for later use.\n",
    "\n",
    "---\n",
    "\n",
    "### Tools and Libraries 🛠️\n",
    "- **`feedparser`**: For parsing the RSS feed.\n",
    "- **`pandas`**: For data manipulation and cleaning.\n",
    "- **CSV Files**: For storing the cleaned data.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion 📊\n",
    "\n",
    "By the end of this project, you will have a fully functional RSS feed data pipeline that extracts, analyzes, and stores BBC News articles, which can serve as the foundation for further data analysis, machine learning, or visualization.\n",
    "\n",
    "Feel free to dive into the individual steps below to learn more about each part of the process!\n",
    "\n",
    "---\n",
    "\n",
    "> **Pro Tip**: You can extend this project to other news sources by modifying the RSS feed URL, and even build more sophisticated analysis with additional libraries like **matplotlib** or **seaborn** for data visualization. 💡\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76178fb6-d134-4fac-af06-ddada7f27602",
   "metadata": {},
   "source": [
    "# Step 0: Install the `feedparser` Library 📦\n",
    "\n",
    "Before we can begin parsing RSS feeds from sources like BBC News, we need to install the necessary Python library: **`feedparser`**. This library makes it easy to fetch and parse RSS feed data.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Install `feedparser`? 🤔\n",
    "\n",
    "- 🧩 **Simple Parsing**: `feedparser` provides a simple API to work with RSS and Atom feeds.\n",
    "- ⚡ **Fast**: It allows you to quickly extract information like titles, links, and publication dates.\n",
    "- 📚 **Widely Used**: `feedparser` is a reliable and widely adopted library for handling feed data in Python.\n",
    "\n",
    "---\n",
    "\n",
    "### Code: Installing `feedparser` with pip 🧪\n",
    "\n",
    "If you're working in a Jupyter notebook or a Python environment, run the following command to install `feedparser`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0510a078-4ed3-4611-aa2b-def0105869c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the feedparser library which is used for parsing RSS and Atom feeds\n",
    "# The library makes it easy to access and process web feeds in Python\n",
    "\n",
    "#!pip install feedparser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611059f8-e553-4842-845f-b0a1a1b59894",
   "metadata": {},
   "source": [
    "# Step 1: Importing Libraries for Data Manipulation and RSS Feed Parsing 🧰\n",
    "\n",
    "In this first step, we begin by importing the essential libraries that will allow us to manipulate data and parse the RSS feed efficiently. These libraries will help us handle the RSS feed and process the extracted data in a structured format.\n",
    "\n",
    "### Libraries 📚\n",
    "\n",
    "- **`pandas`**: A powerful library for **data manipulation** and storing the extracted information in a structured format (such as DataFrames), which is ideal for handling large datasets.\n",
    "- **`feedparser`**: A library designed for **parsing RSS feeds**, enabling easy extraction of data like article titles, links, and publication dates.\n",
    "- **`urllib.parse`**: Provides **URL handling functions** and parsing utilities, useful for handling complex URLs and making sure we can work with them correctly.\n",
    "\n",
    "---\n",
    "\n",
    "### Code: Importing Libraries 🖥️\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f915fa7a-2d76-40a8-8ac0-26d6982ca3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for data manipulation and RSS feed parsing\n",
    "import pandas as pd\n",
    "import feedparser\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56820da0-2e8e-4383-8980-d0cb8e8915c9",
   "metadata": {},
   "source": [
    "# Step 2: RSS Feed Parsing and Data Extraction from BBC News 📰\n",
    "\n",
    "Welcome to the **RSS Feed Parsing** and **Data Extraction** tutorial! In this notebook, we’ll walk through the process of parsing an RSS feed from **BBC News**, extracting valuable data such as titles, links, and publication dates, and saving this information into a **structured CSV file** for future analysis or reporting. 🌟\n",
    "\n",
    "---\n",
    "\n",
    "## Overview 📊\n",
    "\n",
    "Our main objectives in this notebook are:\n",
    "1. **🚀 Parse an RSS feed** using Python's powerful `feedparser` library.\n",
    "2. **🔍 Extract important data fields** from the feed like titles, links, publication dates, and categories.\n",
    "3. **💾 Save the extracted data** in a CSV file for easy access and analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## Parsing the RSS Feed 🔍\n",
    "\n",
    "Now, let’s begin by parsing the RSS feed from **BBC News**. Using the `feedparser` library, we will easily fetch and parse the data, giving us structured access to the headlines, publication dates, and more.\n",
    "\n",
    "### Step-by-step Procedure 📋\n",
    "\n",
    "1. **Import the necessary libraries** 🔧\n",
    "2. **Define the URL** of the **BBC News RSS feed** 🌐\n",
    "3. **Parse the RSS feed** with the `feedparser.parse()` method 📥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "881ad105-43b1-4a4d-aa06-b7103d34ec2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to new_bbc_news_feed.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Parse the RSS feed\n",
    "rss_url = \"https://feeds.bbci.co.uk/news/rss.xml\"\n",
    "feed = feedparser.parse(rss_url)\n",
    "\n",
    "# Step 2: Extract relevant data\n",
    "data = []\n",
    "for entry in feed.entries:\n",
    "    # Extract section from URL (e.g., 'sport', 'news', etc.)\n",
    "    path_parts = urlparse(entry.link).path.strip(\"/\").split(\"/\")\n",
    "    url_section = path_parts[0] if path_parts else None\n",
    "\n",
    "    data.append({\n",
    "        \"title\": entry.title,\n",
    "        \"link\": entry.link,\n",
    "        \"published\": entry.published,\n",
    "        \"summary\": entry.summary,\n",
    "        \"category\": url_section  # Extracted from URL\n",
    "    })\n",
    "\n",
    "# Step 3: Create a DataFrame\n",
    "new_data = pd.DataFrame(data)\n",
    "\n",
    "# Step 4: Save to CSV\n",
    "new_data.to_csv(\"new_bbc_news_feed.csv\", index=False)\n",
    "\n",
    "print(\"Data saved to new_bbc_news_feed.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7334f0fa-24db-49e3-b795-c7f6f092e097",
   "metadata": {},
   "source": [
    "# Step 3: Accessing the DataFrame Head 🔑\n",
    "\n",
    "In this step, we will inspect the **first few rows** of our DataFrame named `new_data`. We'll use the `head()` function to quickly view the top rows of the dataset, which helps us understand its structure before accessing specific columns like **\"category\"**.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Use the `head()` Function? 🤔\n",
    "\n",
    "The `head()` function in Pandas is an easy way to preview the first few rows of a DataFrame. It's particularly useful when you want to:\n",
    "\n",
    "- 🧐 **Inspect the Structure**: Get a sense of the columns and data types.\n",
    "- 🔍 **Verify Data Loading**: Confirm that the dataset has been loaded correctly.\n",
    "- 📊 **Quick Exploration**: View a snapshot of the data to plan next steps.\n",
    "\n",
    "**Advantages**:\n",
    "- 💡 **Quick Preview**: See the top rows instantly without loading the entire dataset.\n",
    "- ⚡ **Efficient**: Ideal for large datasets where viewing the entire DataFrame might not be feasible.\n",
    "- ✅ **Helps in Debugging**: Quickly check for missing data or inconsistencies.\n",
    "\n",
    "---\n",
    "\n",
    "### Code: Accessing the DataFrame Head 📋\n",
    "\n",
    "To view the first few rows of the `new_data` DataFrame, use the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af78aada-310e-4e06-aff8-e75c5d3f98fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>published</th>\n",
       "      <th>summary</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UK and India agree trade deal after three year...</td>\n",
       "      <td>https://www.bbc.com/news/articles/c5y6y90e5vzo</td>\n",
       "      <td>Tue, 06 May 2025 15:21:13 GMT</td>\n",
       "      <td>The deal will improve access for UK whisky and...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Labour MPs' rage over election results simmers on</td>\n",
       "      <td>https://www.bbc.com/news/articles/ckg1rgr25e7o</td>\n",
       "      <td>Tue, 06 May 2025 13:52:00 GMT</td>\n",
       "      <td>Anger over the party's drubbing in last week's...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>King and Queen unveil Coronation portraits</td>\n",
       "      <td>https://www.bbc.com/news/articles/cd020z0dl2eo</td>\n",
       "      <td>Tue, 06 May 2025 14:55:53 GMT</td>\n",
       "      <td>The two portraits will be on display at the ga...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Deliveroo's takeover by US rival shows UK stil...</td>\n",
       "      <td>https://www.bbc.com/news/articles/cvgp22d2kexo</td>\n",
       "      <td>Tue, 06 May 2025 09:08:45 GMT</td>\n",
       "      <td>The takeover by a US firm shows the differing ...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany's Merz wins vote for chancellor after ...</td>\n",
       "      <td>https://www.bbc.com/news/articles/cvgp22zlrgko</td>\n",
       "      <td>Tue, 06 May 2025 15:17:08 GMT</td>\n",
       "      <td>Conservative leader Friedrich Merz has won a p...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  UK and India agree trade deal after three year...   \n",
       "1  Labour MPs' rage over election results simmers on   \n",
       "2         King and Queen unveil Coronation portraits   \n",
       "3  Deliveroo's takeover by US rival shows UK stil...   \n",
       "4  Germany's Merz wins vote for chancellor after ...   \n",
       "\n",
       "                                             link  \\\n",
       "0  https://www.bbc.com/news/articles/c5y6y90e5vzo   \n",
       "1  https://www.bbc.com/news/articles/ckg1rgr25e7o   \n",
       "2  https://www.bbc.com/news/articles/cd020z0dl2eo   \n",
       "3  https://www.bbc.com/news/articles/cvgp22d2kexo   \n",
       "4  https://www.bbc.com/news/articles/cvgp22zlrgko   \n",
       "\n",
       "                       published  \\\n",
       "0  Tue, 06 May 2025 15:21:13 GMT   \n",
       "1  Tue, 06 May 2025 13:52:00 GMT   \n",
       "2  Tue, 06 May 2025 14:55:53 GMT   \n",
       "3  Tue, 06 May 2025 09:08:45 GMT   \n",
       "4  Tue, 06 May 2025 15:17:08 GMT   \n",
       "\n",
       "                                             summary category  \n",
       "0  The deal will improve access for UK whisky and...     news  \n",
       "1  Anger over the party's drubbing in last week's...     news  \n",
       "2  The two portraits will be on display at the ga...     news  \n",
       "3  The takeover by a US firm shows the differing ...     news  \n",
       "4  Conservative leader Friedrich Merz has won a p...     news  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first five rows of the DataFrame for inspection\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698bad19-06b9-4a0d-b123-b1a22f5058be",
   "metadata": {},
   "source": [
    "# Step 4: Counting Non-Null Values in the DataFrame 📊\n",
    "\n",
    "In this step, we will use the `count()` function to determine the number of **non-null values** in each column of our `new_data` DataFrame. This helps us to assess the completeness of our dataset and identify any missing or null values.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Use `count()`? 🤔\n",
    "\n",
    "The `count()` function is a simple and efficient way to get the number of valid (non-null) entries in each column. It allows us to:\n",
    "\n",
    "- 📊 **Verify Data Integrity**: Ensure that we have no missing or null values in critical columns.\n",
    "- 🕵️‍♂️ **Identify Missing Data**: Quickly spot columns that may have missing or incomplete data.\n",
    "- ⚡ **Efficient Check**: Rather than inspecting the entire dataset, `count()` gives us a quick summary of the data.\n",
    "\n",
    "---\n",
    "\n",
    "### Code: Using `count()` to Check Non-Null Values 📋\n",
    "\n",
    "To count the non-null values in each column of the `new_data` DataFrame, use the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50dfc7b1-fbde-4351-a98c-9a00b20b29b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title        32\n",
       "link         32\n",
       "published    32\n",
       "summary      32\n",
       "category     32\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of non-null values in each column of the new_data DataFrame\n",
    "# This provides a quick overview of data completeness across all columns\n",
    "new_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e79bf5-7569-48bd-84cb-9b2663964db8",
   "metadata": {},
   "source": [
    "# Step 5: Loading and Copying the Dataset 📥\n",
    "\n",
    "In this step, we will load the dataset from a CSV file into a **Pandas DataFrame** and create a deep copy of the DataFrame to preserve the original data. This allows us to manipulate the data without altering the original dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Create a Deep Copy? 🤔\n",
    "\n",
    "A deep copy creates an independent copy of the DataFrame, ensuring that any changes made to the new DataFrame do not affect the original data. This is useful for maintaining the integrity of the dataset throughout the analysis process.\n",
    "\n",
    "**Advantages**:\n",
    "- 🛡️ **Data Preservation**: The original dataset remains intact, which is crucial for reproducibility.\n",
    "- 🔄 **Safe Modifications**: Any modifications made to the new DataFrame won’t impact the original data.\n",
    "- 💡 **Versatility**: You can experiment freely with the copied data without worrying about losing the original.\n",
    "\n",
    "---\n",
    "\n",
    "### Code: Loading the Dataset and Creating a Deep Copy 📋\n",
    "\n",
    "To load the CSV file into a DataFrame and create a deep copy, use the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58cfbcff-c373-4380-a747-a9bad7902716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from CSV file into a pandas DataFrame\n",
    "data = pd.read_csv('old_bbc_news_feed .csv')\n",
    "# Create a deep copy of the DataFrame to preserve the original data\n",
    "old_data = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acc49f6-b5a7-4287-a249-3ad7c6f2df48",
   "metadata": {},
   "source": [
    "# Step 6: Viewing the First Few Rows of the DataFrame 🔍\n",
    "\n",
    "In this step, we will view the first few rows of the `old_data` DataFrame using the `head()` function. This allows us to quickly inspect the dataset and get a sense of its structure and contents.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Use `head()`? 🤔\n",
    "\n",
    "The `head()` function is a useful tool for:\n",
    "- 🧐 **Previewing the Data**: It provides a snapshot of the first few rows, helping you understand the dataset's structure without loading the entire data.\n",
    "- 🧑‍💻 **Exploratory Data Analysis (EDA)**: It helps you start your analysis by checking the initial records of the dataset.\n",
    "- ⚡ **Quick Verification**: Verifying that the data was loaded correctly and that it contains the expected columns and values.\n",
    "\n",
    "By default, `head()` returns the first **5 rows** of the DataFrame, but you can specify the number of rows to display.\n",
    "\n",
    "---\n",
    "\n",
    "### Code: Viewing the First Few Rows of the `old_data` DataFrame 📋\n",
    "\n",
    "To view the first 5 rows of the `old_data` DataFrame, use the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "42c61eda-f218-4058-8172-73f0237261f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>published</th>\n",
       "      <th>summary</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UK and India agree trade deal after three year...</td>\n",
       "      <td>https://www.bbc.com/news/articles/c5y6y90e5vzo</td>\n",
       "      <td>Tue, 06 May 2025 15:21:13 GMT</td>\n",
       "      <td>The deal will improve access for UK whisky and...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Labour MPs' rage over election results simmers on</td>\n",
       "      <td>https://www.bbc.com/news/articles/ckg1rgr25e7o</td>\n",
       "      <td>Tue, 06 May 2025 13:52:00 GMT</td>\n",
       "      <td>Anger over the party's drubbing in last week's...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Germany's Merz wins vote for chancellor after ...</td>\n",
       "      <td>https://www.bbc.com/news/articles/cvgp22zlrgko</td>\n",
       "      <td>Tue, 06 May 2025 15:17:08 GMT</td>\n",
       "      <td>Conservative leader Friedrich Merz has won a p...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>King and Queen unveil Coronation portraits</td>\n",
       "      <td>https://www.bbc.com/news/articles/cd020z0dl2eo</td>\n",
       "      <td>Tue, 06 May 2025 14:55:53 GMT</td>\n",
       "      <td>The two portraits will be on display at the ga...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Deliveroo's takeover by US rival shows UK stil...</td>\n",
       "      <td>https://www.bbc.com/news/articles/cvgp22d2kexo</td>\n",
       "      <td>Tue, 06 May 2025 09:08:45 GMT</td>\n",
       "      <td>The takeover by a US firm shows the differing ...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  UK and India agree trade deal after three year...   \n",
       "1  Labour MPs' rage over election results simmers on   \n",
       "2  Germany's Merz wins vote for chancellor after ...   \n",
       "3         King and Queen unveil Coronation portraits   \n",
       "4  Deliveroo's takeover by US rival shows UK stil...   \n",
       "\n",
       "                                             link  \\\n",
       "0  https://www.bbc.com/news/articles/c5y6y90e5vzo   \n",
       "1  https://www.bbc.com/news/articles/ckg1rgr25e7o   \n",
       "2  https://www.bbc.com/news/articles/cvgp22zlrgko   \n",
       "3  https://www.bbc.com/news/articles/cd020z0dl2eo   \n",
       "4  https://www.bbc.com/news/articles/cvgp22d2kexo   \n",
       "\n",
       "                       published  \\\n",
       "0  Tue, 06 May 2025 15:21:13 GMT   \n",
       "1  Tue, 06 May 2025 13:52:00 GMT   \n",
       "2  Tue, 06 May 2025 15:17:08 GMT   \n",
       "3  Tue, 06 May 2025 14:55:53 GMT   \n",
       "4  Tue, 06 May 2025 09:08:45 GMT   \n",
       "\n",
       "                                             summary category  \n",
       "0  The deal will improve access for UK whisky and...     news  \n",
       "1  Anger over the party's drubbing in last week's...     news  \n",
       "2  Conservative leader Friedrich Merz has won a p...     news  \n",
       "3  The two portraits will be on display at the ga...     news  \n",
       "4  The takeover by a US firm shows the differing ...     news  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first five rows of the old_data DataFrame to inspect its structure and content\n",
    "old_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4623229-1617-466a-8638-7e71e76ad5f2",
   "metadata": {},
   "source": [
    "# Step 7: Counting Non-Null Values in the DataFrame 📊\n",
    "\n",
    "In this step, we will use the `count()` function to determine the number of **non-null values** in each column of the `old_data` DataFrame. This helps us assess the completeness of the dataset and identify any missing values.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Use `count()`? 🤔\n",
    "\n",
    "The `count()` function is a simple yet powerful tool for:\n",
    "- 🕵️‍♂️ **Identifying Missing Data**: It counts only non-null (valid) entries in each column, helping us spot any missing data.\n",
    "- 📊 **Verifying Data Integrity**: Ensures that each column contains valid, usable data before starting deeper analysis.\n",
    "- 🔄 **Data Exploration**: Provides insight into how many records are available for each column, especially useful for large datasets.\n",
    "\n",
    "By default, `count()` returns the number of **non-null values** for each column in the DataFrame.\n",
    "\n",
    "---\n",
    "\n",
    "### Code: Counting Non-Null Values in the `old_data` DataFrame 📋\n",
    "\n",
    "To count the number of non-null values in each column of `old_data`, use the following code:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "147a62f4-1a0b-4447-b7d2-e0aef359519b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title        32\n",
       "link         32\n",
       "published    32\n",
       "summary      32\n",
       "category     32\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of non-null values in each column of the old_data DataFrame\n",
    "# This provides a quick overview of data completeness\n",
    "old_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46670a0-d09a-4ad3-9495-1c69074d9335",
   "metadata": {},
   "source": [
    "# Step 8: Combining Old and New Data 📦➕🗃️\n",
    "\n",
    "In this step, we combine the previously loaded **old data** with the newly extracted **BBC RSS feed data**. This is essential for maintaining a consolidated and up-to-date dataset. After combining, we ensure the dataset remains clean by removing duplicate entries based on the **link** field.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Combine Datasets? 🤔\n",
    "\n",
    "- 🔁 **Incremental Updates**: RSS feeds often provide only the latest entries. Merging with historical data preserves the full record.\n",
    "- 🧩 **Unified Dataset**: Combining old and new data provides a single, comprehensive DataFrame for analysis.\n",
    "- 🧼 **Data Integrity**: Removing duplicates prevents repeated entries in visualizations, summaries, or machine learning inputs.\n",
    "\n",
    "---\n",
    "\n",
    "### Code: Merge and Deduplicate DataFrames 🧪\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c271a1c9-28b5-4969-b996-55a29d04a788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine old and new data\n",
    "combined_data = pd.concat([old_data, new_data], ignore_index=True)\n",
    "\n",
    "# Remove duplicates (e.g., by title or link)\n",
    "combined_data = combined_data.drop_duplicates(subset=\"link\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47449efa-d55f-4481-9ca9-c525d7ebf118",
   "metadata": {},
   "source": [
    "# Step 9: Previewing the Combined Dataset 🧾🔍\n",
    "\n",
    "After merging the old and new BBC News data, it’s important to preview the result to ensure that the combination and deduplication steps were successful. We use the `head()` function to view the first few rows of the `combined_data` DataFrame.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Preview the Data? 🤔\n",
    "\n",
    "- 🧠 **Quick Sanity Check**: Verify that the data looks as expected after combining.\n",
    "- ✅ **Validate Deduplication**: Ensure duplicate articles (e.g., same links) were removed properly.\n",
    "- 🔍 **Inspect Structure**: Confirm column names and data formats are consistent.\n",
    "\n",
    "---\n",
    "\n",
    "### Code: View the Top Rows of Combined Data 🧪\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd66d11-bb7e-44dd-8c37-05da55fa6b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbc421a-55ba-497c-8560-016d09ed6ec3",
   "metadata": {},
   "source": [
    "# Step 10: Counting Non-Null Values in the Combined Dataset 📊\n",
    "\n",
    "After merging and cleaning the dataset, it’s crucial to verify that all columns contain the expected number of valid (non-null) values. We use the `count()` function to count the number of **non-null** values in each column of the `combined_data` DataFrame.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Use `count()`? 🤔\n",
    "\n",
    "- 🧑‍💻 **Verify Data Integrity**: Ensure that the dataset has no missing or incomplete values.\n",
    "- 📈 **Understand the Dataset**: Confirm that each column contains a full set of valid data for analysis.\n",
    "- 🧹 **Pre-Analysis Cleanliness**: Helps in identifying columns with missing values that may need to be addressed before further analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### Code: Count Non-Null Values in the Combined Data 🧪\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c244f6ec-a0e4-4800-a86e-76b251aba689",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c79742-200f-40c8-bdef-421639b64ff3",
   "metadata": {},
   "source": [
    "# Step 11: Saving the Combined Data to a CSV File 💾\n",
    "\n",
    "Now that we've successfully combined and cleaned the BBC News data, it’s time to save the `combined_data` DataFrame to a CSV file. This allows us to preserve the data for future use, analysis, or sharing.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Save to CSV? 🤔\n",
    "\n",
    "- 📦 **Persistent Storage**: Storing data in a CSV file allows us to easily access and use it in the future without having to reload and reprocess the original data.\n",
    "- 📈 **Interoperability**: CSV files are widely supported by many tools and programming languages, making them easy to share and use in various applications.\n",
    "- 🔒 **Data Safety**: Saving the dataset ensures that you don't lose the work done up to this point in case the environment is cleared or the data is lost.\n",
    "\n",
    "---\n",
    "\n",
    "### Code: Saving the Combined Data 🧪\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47528433-3b32-4504-8579-76ec6a2d171f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the submission DataFrame to a CSV file without the index\n",
    "combined_data.to_csv('old_bbc_news_feed.csv', index=False)\n",
    "\n",
    "# Confirm the file has been saved (optional)\n",
    "print(\"Combined Data file has been saved as 'old_bbc_news_feed.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbbf447-b806-4ade-b902-92525e0c20bb",
   "metadata": {},
   "source": [
    "# 👋 Muhammad Muhammad Mudasar Sabir\n",
    "\n",
    "I’m a Machine Learning and Deep Learning enthusiast with a strong interest in Computer Vision and Generative AI. I enjoy solving real-world problems using intelligent, data-driven approaches. My focus areas include:\n",
    "\n",
    "- 🤖 Machine Learning & Deep Learning  \n",
    "- 🧠 Computer Vision & Generative Models  \n",
    "- 📊 Data Analysis & Feature Engineering  \n",
    "- 🚀 Model Evaluation & Deployment  \n",
    "\n",
    "---\n",
    "\n",
    "## 🔗 Connect with Me\n",
    "\n",
    "- 🧠 **Kaggle**: [https://www.kaggle.com/mudasarsabir](https://www.kaggle.com/mudasarsabir)  \n",
    "- 💻 **GitHub**: [https://github.com/mudasarsabir](https://github.com/mudasarsabir)  \n",
    "- 🔗 **LinkedIn**: [https://www.linkedin.com/in/mudasarsabir](https://www.linkedin.com/in/mudasarsabir)  \n",
    "- 🌐 **Portfolio**: [https://muddasarsabir.netlify.app](https://muddasarsabir.netlify.app)\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Featured Project\n",
    "\n",
    "### 🎯 Titanic Survival Prediction - Decision Tree\n",
    "- GitHub: [View Repository](https://github.com/mudasarsabir)  \n",
    "- Kaggle: [View Notebook](https://www.kaggle.com/mudasarsabir)  \n",
    "\n",
    "A beginner-friendly ML project applying Decision Tree classification to predict Titanic passenger survival, including EDA, preprocessing, and model evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## 📬 Get in Touch\n",
    "\n",
    "I'm open to collaboration, research, and AI-focused opportunities. Let’s connect!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
